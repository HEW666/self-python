# PySpark准备
## 安装PySpark库
```python
pip install pyspark
```
## 知识点
<u>PySpark的编程模型</u> ：**数据输入**、**数据处理计算**、**数据输出**
> PySpark的功能都是从SparkContext对象作为开始
> SparkContext类对象，是PySpark编程中一切功能的入口
## 数据输入
> 通过SparkContext类对象的成员方法完成数据的读取操作读取后得到RDD类对象
## 数据处理计算
> 通过RDD类对象的成员方法完成各种数据计算的需求
## 数据输出
> 将处理完成后的RDD对象调用各种成员方法完成写出文件、转换为list等操作

# 基础应用
```python
from pyspark import SparkConf, SparkContext  
  
# 创建SparkConf对象  
conf = SparkConf().setMaster("local[*]").setAppName("test_spark_app")  
# 基于SparkConf类对象创建SparkContext对象  
sc = SparkContext(conf=conf)  
# 打印PySpark的运行版本  
print(sc.version)  
# 停止SparkContext对象的运行（停止PySpark程序）  
sc.stop()
```
